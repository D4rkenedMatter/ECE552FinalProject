{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"DataCleaner\") \\\n",
    "    .config(\"spark.mongodb.input.uri\", \"mongodb://127.0.0.1/ECE552FinalProject.TrafficData\") \\\n",
    "    .config(\"spark.mongodb.output.uri\", \"mongodb://127.0.0.1/ECE552FinalProject.TrafficData\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in parquet\n",
    "parquetDF = spark.read.parquet(\"TrafficAccidentsData.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_abbrev = ['AL','AK','AZ','AR','CA','CO','CT','DE','FL','GA','HI','ID','IL','IN','IA','KS','KY','LA','ME','MD','MA','MI','MN','MS','MO','MT','NE','NV','NH','NJ','NM','NY','NC','ND','OH','OK','OR','PA','RI','SC','SD','TN','TX','UT','VT','VA','WA','WV','WI','WY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = \"[{ $group : { _id : '$State'}}]\"\n",
    "#[{$group:{_id: \"$State\"}}]\n",
    "df = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").option(\"pipeline\", pipeline).load()\n",
    "print(df.count())\n",
    "#This does not work as intended as spark is trying to do this job with multiple partitions of the data in mongo, \n",
    "#returning multiple identical tags it appears, if you do this exact pipeline command in mongoDB compass, it works fine\n",
    "#have to do some extra aggregation in spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pipeline = \"[{ $group : { _id : '$Astronomical_Twilight', count:{$sum:1}}}, {$sort:{_id:1}}]\"\n",
    "df = spark.read.format(\"mongo\").option(\"pipeline\", pipeline).load()\n",
    "df = df.groupby(\"_id\").sum()\n",
    "df = df.withColumnRenamed(\"_id\",\"AccidentAstronomicalTwilight\").withColumnRenamed(\"sum(count)\",\"AccidentCount\")\n",
    "df = df.orderBy(\"AccidentAstronomicalTwilight\")\n",
    "print(df.count())\n",
    "df.agg({'AccidentCount': 'sum'}).show()\n",
    "df.show(49)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.format(\"mongo\").option(\"collection\", \"AccidentCountByAstronomicalTwilight\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
